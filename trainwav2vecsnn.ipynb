{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815300a3-bd54-4207-80dd-f8dbb1b85579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step 1/59, Loss: 2.3147\n",
      "Epoch 1/30, Step 11/59, Loss: 2.2564\n",
      "Epoch 1/30, Step 21/59, Loss: 2.3417\n",
      "Epoch 1/30, Step 31/59, Loss: 2.3026\n",
      "Epoch 1/30, Step 41/59, Loss: 2.3153\n",
      "Epoch 1/30, Step 51/59, Loss: 2.3002\n",
      "Epoch 1, Validation Loss: 2.2968, Validation Accuracy: 13.67%\n",
      "Epoch 2/30, Step 1/59, Loss: 2.2895\n",
      "Epoch 2/30, Step 11/59, Loss: 2.3030\n",
      "Epoch 2/30, Step 21/59, Loss: 2.2979\n",
      "Epoch 2/30, Step 31/59, Loss: 2.3080\n",
      "Epoch 2/30, Step 41/59, Loss: 2.3007\n",
      "Epoch 2/30, Step 51/59, Loss: 2.3034\n",
      "Epoch 2, Validation Loss: 2.2913, Validation Accuracy: 16.33%\n",
      "Epoch 3/30, Step 1/59, Loss: 2.2953\n",
      "Epoch 3/30, Step 11/59, Loss: 2.2981\n",
      "Epoch 3/30, Step 21/59, Loss: 2.3241\n",
      "Epoch 3/30, Step 31/59, Loss: 2.2908\n",
      "Epoch 3/30, Step 41/59, Loss: 2.2896\n",
      "Epoch 3/30, Step 51/59, Loss: 2.2953\n",
      "Epoch 3, Validation Loss: 2.2957, Validation Accuracy: 13.00%\n",
      "Epoch 4/30, Step 1/59, Loss: 2.2971\n",
      "Epoch 4/30, Step 11/59, Loss: 2.2985\n",
      "Epoch 4/30, Step 21/59, Loss: 2.3003\n",
      "Epoch 4/30, Step 31/59, Loss: 2.3058\n",
      "Epoch 4/30, Step 41/59, Loss: 2.3026\n",
      "Epoch 4/30, Step 51/59, Loss: 2.3103\n",
      "Epoch 4, Validation Loss: 2.2942, Validation Accuracy: 14.33%\n",
      "Epoch 5/30, Step 1/59, Loss: 2.3006\n",
      "Epoch 5/30, Step 11/59, Loss: 2.2978\n",
      "Epoch 5/30, Step 21/59, Loss: 2.3104\n",
      "Epoch 5/30, Step 31/59, Loss: 2.3090\n",
      "Epoch 5/30, Step 41/59, Loss: 2.3044\n",
      "Epoch 5/30, Step 51/59, Loss: 2.3024\n",
      "Epoch 5, Validation Loss: 2.2925, Validation Accuracy: 14.50%\n",
      "Epoch 6/30, Step 1/59, Loss: 2.2935\n",
      "Epoch 6/30, Step 11/59, Loss: 2.3091\n",
      "Epoch 6/30, Step 21/59, Loss: 2.3009\n",
      "Epoch 6/30, Step 31/59, Loss: 2.3115\n",
      "Epoch 6/30, Step 41/59, Loss: 2.3041\n",
      "Epoch 6/30, Step 51/59, Loss: 2.3113\n",
      "Epoch 6, Validation Loss: 2.2938, Validation Accuracy: 13.67%\n",
      "Epoch 7/30, Step 1/59, Loss: 2.2958\n",
      "Epoch 7/30, Step 11/59, Loss: 2.3270\n",
      "Epoch 7/30, Step 21/59, Loss: 2.2982\n",
      "Epoch 7/30, Step 31/59, Loss: 2.2944\n",
      "Epoch 7/30, Step 41/59, Loss: 2.3194\n",
      "Epoch 7/30, Step 51/59, Loss: 2.2986\n",
      "Epoch 7, Validation Loss: 2.2922, Validation Accuracy: 14.67%\n",
      "Epoch 8/30, Step 1/59, Loss: 2.2981\n",
      "Epoch 8/30, Step 11/59, Loss: 2.3174\n",
      "Epoch 8/30, Step 21/59, Loss: 2.3018\n",
      "Epoch 8/30, Step 31/59, Loss: 2.2920\n",
      "Epoch 8/30, Step 41/59, Loss: 2.3428\n",
      "Epoch 8/30, Step 51/59, Loss: 2.2975\n",
      "Epoch 8, Validation Loss: 2.2874, Validation Accuracy: 18.17%\n",
      "Epoch 9/30, Step 1/59, Loss: 2.2999\n",
      "Epoch 9/30, Step 11/59, Loss: 2.3685\n",
      "Epoch 9/30, Step 21/59, Loss: 2.3022\n",
      "Epoch 9/30, Step 31/59, Loss: 2.2977\n",
      "Epoch 9/30, Step 41/59, Loss: 2.3060\n",
      "Epoch 9/30, Step 51/59, Loss: 2.2984\n",
      "Epoch 9, Validation Loss: 2.2886, Validation Accuracy: 15.67%\n",
      "Epoch 10/30, Step 1/59, Loss: 2.2860\n",
      "Epoch 10/30, Step 11/59, Loss: 2.3074\n",
      "Epoch 10/30, Step 21/59, Loss: 2.3010\n",
      "Epoch 10/30, Step 31/59, Loss: 2.3075\n",
      "Epoch 10/30, Step 41/59, Loss: 2.3044\n",
      "Epoch 10/30, Step 51/59, Loss: 2.2945\n",
      "Epoch 10, Validation Loss: 2.2862, Validation Accuracy: 15.83%\n",
      "Epoch 11/30, Step 1/59, Loss: 2.3169\n",
      "Epoch 11/30, Step 11/59, Loss: 2.2850\n",
      "Epoch 11/30, Step 21/59, Loss: 2.2774\n",
      "Epoch 11/30, Step 31/59, Loss: 2.2984\n",
      "Epoch 11/30, Step 41/59, Loss: 2.2922\n",
      "Epoch 11/30, Step 51/59, Loss: 2.2945\n",
      "Epoch 11, Validation Loss: 2.2893, Validation Accuracy: 15.67%\n",
      "Epoch 12/30, Step 1/59, Loss: 2.2895\n",
      "Epoch 12/30, Step 11/59, Loss: 2.3050\n",
      "Epoch 12/30, Step 21/59, Loss: 2.2920\n",
      "Epoch 12/30, Step 31/59, Loss: 2.2868\n",
      "Epoch 12/30, Step 41/59, Loss: 2.3023\n",
      "Epoch 12/30, Step 51/59, Loss: 2.2900\n",
      "Epoch 12, Validation Loss: 2.2868, Validation Accuracy: 15.50%\n",
      "Epoch 13/30, Step 1/59, Loss: 2.3006\n",
      "Epoch 13/30, Step 11/59, Loss: 2.2999\n",
      "Epoch 13/30, Step 21/59, Loss: 2.2915\n",
      "Epoch 13/30, Step 31/59, Loss: 2.2888\n",
      "Epoch 13/30, Step 41/59, Loss: 2.3058\n",
      "Epoch 13/30, Step 51/59, Loss: 2.3126\n",
      "Epoch 13, Validation Loss: 2.2891, Validation Accuracy: 15.17%\n",
      "Epoch 14/30, Step 1/59, Loss: 2.2859\n",
      "Epoch 14/30, Step 11/59, Loss: 2.3091\n",
      "Epoch 14/30, Step 21/59, Loss: 2.3147\n",
      "Epoch 14/30, Step 31/59, Loss: 2.2955\n",
      "Epoch 14/30, Step 41/59, Loss: 2.3123\n",
      "Epoch 14/30, Step 51/59, Loss: 2.3076\n",
      "Epoch 14, Validation Loss: 2.2907, Validation Accuracy: 14.33%\n",
      "Epoch 15/30, Step 1/59, Loss: 2.2973\n",
      "Epoch 15/30, Step 11/59, Loss: 2.3083\n",
      "Epoch 15/30, Step 21/59, Loss: 2.3000\n",
      "Epoch 15/30, Step 31/59, Loss: 2.2752\n",
      "Epoch 15/30, Step 41/59, Loss: 2.3024\n",
      "Epoch 15/30, Step 51/59, Loss: 2.3574\n",
      "Epoch 15, Validation Loss: 2.2908, Validation Accuracy: 13.50%\n",
      "Epoch 16/30, Step 1/59, Loss: 2.2891\n",
      "Epoch 16/30, Step 11/59, Loss: 2.2794\n",
      "Epoch 16/30, Step 21/59, Loss: 2.3100\n",
      "Epoch 16/30, Step 31/59, Loss: 2.3206\n",
      "Epoch 16/30, Step 41/59, Loss: 2.2982\n",
      "Epoch 16/30, Step 51/59, Loss: 2.3139\n",
      "Epoch 16, Validation Loss: 2.2873, Validation Accuracy: 14.17%\n",
      "Epoch 17/30, Step 1/59, Loss: 2.3085\n",
      "Epoch 17/30, Step 11/59, Loss: 2.2918\n",
      "Epoch 17/30, Step 21/59, Loss: 2.3258\n",
      "Epoch 17/30, Step 31/59, Loss: 2.2998\n",
      "Epoch 17/30, Step 41/59, Loss: 2.2801\n",
      "Epoch 17/30, Step 51/59, Loss: 2.2927\n",
      "Epoch 17, Validation Loss: 2.2781, Validation Accuracy: 16.67%\n",
      "Epoch 18/30, Step 1/59, Loss: 2.3204\n",
      "Epoch 18/30, Step 11/59, Loss: 2.3144\n",
      "Epoch 18/30, Step 21/59, Loss: 2.2818\n",
      "Epoch 18/30, Step 31/59, Loss: 2.2624\n",
      "Epoch 18/30, Step 41/59, Loss: 2.3025\n",
      "Epoch 18/30, Step 51/59, Loss: 2.2865\n",
      "Epoch 18, Validation Loss: 2.2852, Validation Accuracy: 13.83%\n",
      "Epoch 19/30, Step 1/59, Loss: 2.3126\n",
      "Epoch 19/30, Step 11/59, Loss: 2.2709\n",
      "Epoch 19/30, Step 21/59, Loss: 2.3141\n",
      "Epoch 19/30, Step 31/59, Loss: 2.3154\n",
      "Epoch 19/30, Step 41/59, Loss: 2.2926\n",
      "Epoch 19/30, Step 51/59, Loss: 2.2463\n",
      "Epoch 19, Validation Loss: 2.2764, Validation Accuracy: 15.50%\n",
      "Epoch 20/30, Step 1/59, Loss: 2.2573\n",
      "Epoch 20/30, Step 11/59, Loss: 2.2970\n",
      "Epoch 20/30, Step 21/59, Loss: 2.3067\n",
      "Epoch 20/30, Step 31/59, Loss: 2.2650\n",
      "Epoch 20/30, Step 41/59, Loss: 2.2622\n",
      "Epoch 20/30, Step 51/59, Loss: 2.2790\n",
      "Epoch 20, Validation Loss: 2.2808, Validation Accuracy: 13.67%\n",
      "Epoch 21/30, Step 1/59, Loss: 2.2864\n",
      "Epoch 21/30, Step 11/59, Loss: 2.2456\n",
      "Epoch 21/30, Step 21/59, Loss: 2.3222\n",
      "Epoch 21/30, Step 31/59, Loss: 2.3139\n",
      "Epoch 21/30, Step 41/59, Loss: 2.2567\n",
      "Epoch 21/30, Step 51/59, Loss: 2.2482\n",
      "Epoch 21, Validation Loss: 2.2646, Validation Accuracy: 17.33%\n",
      "Epoch 22/30, Step 1/59, Loss: 2.2969\n",
      "Epoch 22/30, Step 11/59, Loss: 2.2766\n",
      "Epoch 22/30, Step 21/59, Loss: 2.2969\n",
      "Epoch 22/30, Step 31/59, Loss: 2.2880\n",
      "Epoch 22/30, Step 41/59, Loss: 2.2945\n",
      "Epoch 22/30, Step 51/59, Loss: 2.2953\n",
      "Epoch 22, Validation Loss: 2.2664, Validation Accuracy: 14.67%\n",
      "Epoch 23/30, Step 1/59, Loss: 2.2812\n",
      "Epoch 23/30, Step 11/59, Loss: 2.2664\n",
      "Epoch 23/30, Step 21/59, Loss: 2.2864\n",
      "Epoch 23/30, Step 31/59, Loss: 2.2880\n",
      "Epoch 23/30, Step 41/59, Loss: 2.2962\n",
      "Epoch 23/30, Step 51/59, Loss: 2.2994\n",
      "Epoch 23, Validation Loss: 2.2701, Validation Accuracy: 14.33%\n",
      "Epoch 24/30, Step 1/59, Loss: 2.3676\n",
      "Epoch 24/30, Step 11/59, Loss: 2.2793\n",
      "Epoch 24/30, Step 21/59, Loss: 2.3138\n",
      "Epoch 24/30, Step 31/59, Loss: 2.2783\n",
      "Epoch 24/30, Step 41/59, Loss: 2.2986\n",
      "Epoch 24/30, Step 51/59, Loss: 2.3271\n",
      "Epoch 24, Validation Loss: 2.2674, Validation Accuracy: 13.33%\n",
      "Epoch 25/30, Step 1/59, Loss: 2.2596\n",
      "Epoch 25/30, Step 11/59, Loss: 2.2953\n",
      "Epoch 25/30, Step 21/59, Loss: 2.3124\n",
      "Epoch 25/30, Step 31/59, Loss: 2.2733\n",
      "Epoch 25/30, Step 41/59, Loss: 2.2628\n",
      "Epoch 25/30, Step 51/59, Loss: 2.2900\n",
      "Epoch 25, Validation Loss: 2.2571, Validation Accuracy: 14.67%\n",
      "Epoch 26/30, Step 1/59, Loss: 2.3117\n",
      "Epoch 26/30, Step 11/59, Loss: 2.2594\n",
      "Epoch 26/30, Step 21/59, Loss: 2.3021\n",
      "Epoch 26/30, Step 31/59, Loss: 2.2508\n",
      "Epoch 26/30, Step 41/59, Loss: 2.2649\n",
      "Epoch 26/30, Step 51/59, Loss: 2.2456\n",
      "Epoch 26, Validation Loss: 2.2471, Validation Accuracy: 15.33%\n",
      "Epoch 27/30, Step 1/59, Loss: 2.2706\n",
      "Epoch 27/30, Step 11/59, Loss: 2.2605\n",
      "Epoch 27/30, Step 21/59, Loss: 2.3068\n",
      "Epoch 27/30, Step 31/59, Loss: 2.2517\n",
      "Epoch 27/30, Step 41/59, Loss: 2.2699\n",
      "Epoch 27/30, Step 51/59, Loss: 2.2933\n",
      "Epoch 27, Validation Loss: 2.2419, Validation Accuracy: 17.17%\n",
      "Epoch 28/30, Step 1/59, Loss: 2.2988\n",
      "Epoch 28/30, Step 11/59, Loss: 2.3193\n",
      "Epoch 28/30, Step 21/59, Loss: 2.2633\n",
      "Epoch 28/30, Step 31/59, Loss: 2.2784\n",
      "Epoch 28/30, Step 41/59, Loss: 2.3117\n",
      "Epoch 28/30, Step 51/59, Loss: 2.2982\n",
      "Epoch 28, Validation Loss: 2.2550, Validation Accuracy: 14.33%\n",
      "Epoch 29/30, Step 1/59, Loss: 2.2604\n",
      "Epoch 29/30, Step 11/59, Loss: 2.2870\n",
      "Epoch 29/30, Step 21/59, Loss: 2.2796\n",
      "Epoch 29/30, Step 31/59, Loss: 2.2920\n",
      "Epoch 29/30, Step 41/59, Loss: 2.2912\n",
      "Epoch 29/30, Step 51/59, Loss: 2.2161\n",
      "Epoch 29, Validation Loss: 2.2449, Validation Accuracy: 15.17%\n",
      "Epoch 30/30, Step 1/59, Loss: 2.3013\n",
      "Epoch 30/30, Step 11/59, Loss: 2.2931\n",
      "Epoch 30/30, Step 21/59, Loss: 2.2722\n",
      "Epoch 30/30, Step 31/59, Loss: 2.2889\n",
      "Epoch 30/30, Step 41/59, Loss: 2.2338\n",
      "Epoch 30/30, Step 51/59, Loss: 2.2783\n",
      "Epoch 30, Validation Loss: 2.2526, Validation Accuracy: 13.83%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define a new combined model\n",
    "class Wav2Vec2AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, feature_extractor_model=\"facebook/wav2vec2-large-960h\"):\n",
    "        super(Wav2Vec2AudioClassifier, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(feature_extractor_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.wav2vec2.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        with torch.no_grad():  # Remove this line if you want to fine-tune Wav2Vec2\n",
    "            features = self.wav2vec2(input_values).last_hidden_state\n",
    "        features = features.mean(dim=1)  # Global average pooling\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(dataset_path, data_type, processor, sampling_rate=16000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file_number in range(1, 51):\n",
    "        filename = f\"data_{file_number}_{data_type}.pkl\"\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                file_data = pickle.load(file)\n",
    "                for waveform, label in file_data:\n",
    "                    input_values = processor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=sampling_rate).input_values\n",
    "                    data.append(input_values)\n",
    "                    labels.append(label)\n",
    "\n",
    "    labels = [x[0] for x in labels]  # Adjust label format if necessary\n",
    "    return data, torch.tensor(labels)\n",
    "\n",
    "# Initialize processor and data loaders\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "dataset_path = 'emi_dataset/'\n",
    "\n",
    "train_data, train_labels = load_data(dataset_path, \"train\", processor)\n",
    "train_dataset = TensorDataset(torch.cat(train_data, dim=0), train_labels)  # Concatenate all data tensors\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validate_data, validate_labels = load_data(dataset_path, \"valid\", processor)\n",
    "validate_dataset = TensorDataset(torch.cat(validate_data, dim=0), validate_labels)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=32)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Wav2Vec2AudioClassifier(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (input_values, labels) in enumerate(train_loader):\n",
    "        input_values, labels = input_values.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop (example)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input_values, labels in validate_loader:\n",
    "        input_values, labels = input_values.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(validate_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(100 * correct / total):.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e845f6-a00d-4125-bd3f-84d214668752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data, test_labels = load_data(dataset_path, \"test\", processor)\n",
    "test_dataset = TensorDataset(torch.cat(test_data, dim=0), test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "for input_values, labels in test_loader:\n",
    "    input_values, labels = input_values.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
