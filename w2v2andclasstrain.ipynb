{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253bb906-aa26-46d1-9e73-38a75a126318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2ForCTC, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085779a8-1177-4598-a0bc-151152757d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No audio files found in directory emi_dataset/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Check if we found any audio files\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m audio_paths:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo audio files found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Now create the dataset\u001b[39;00m\n\u001b[1;32m     12\u001b[0m audio_dataset \u001b[38;5;241m=\u001b[39m AudioDataset(audio_paths, labels, processor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No audio files found in directory emi_dataset/"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "audio_dir = 'emi_dataset/'\n",
    "audio_paths = glob.glob(os.path.join(audio_dir, '*.pkl'))  # Adjust the pattern if your files have a different extension\n",
    "\n",
    "# Check if we found any audio files\n",
    "if not audio_paths:\n",
    "    raise RuntimeError(f\"No audio files found in directory {audio_dir}\")\n",
    "\n",
    "# Now create the dataset\n",
    "audio_dataset = AudioDataset(audio_paths, labels, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b20775-918a-4459-89f1-dd980a6dc700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vi.gade/.local/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"a\" (No such file or directory).\nException raised from get_input_format_context at /__w/_temp/conda_environment_7505053768/conda-bld/torchaudio_1705078604992/work/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x151bd33a9d87 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x151bd335a75f in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x151b6facb904 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x151b6face304 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #4: <unknown function> + 0x3a58e (0x151b65c9f58e in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/_torio_ffmpeg6.so)\nframe #5: <unknown function> + 0x32147 (0x151b65c97147 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/_torio_ffmpeg6.so)\n<omitting python frames>\nframe #11: <unknown function> + 0xf244 (0x151b748d8244 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torchaudio/lib/_torchaudio.so)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m wav2vec2_train_loader \u001b[38;5;241m=\u001b[39m DataLoader(audio_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now you can fine-tune the Wav2Vec2 model\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[43mfine_tune_wav2vec2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav2vec2_train_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Load and prepare data for AudioCNN training\u001b[39;00m\n\u001b[1;32m    104\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memi_dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m, in \u001b[0;36mfine_tune_wav2vec2\u001b[0;34m(train_loader, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model_wav2vec2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (input_values, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     57\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     58\u001b[0m         input_values \u001b[38;5;241m=\u001b[39m input_values\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 87\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_paths[idx]\n\u001b[1;32m     85\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 87\u001b[0m waveform, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[1;32m     88\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(waveform\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy(), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampling_rate\u001b[38;5;241m=\u001b[39msr)\u001b[38;5;241m.\u001b[39minput_values  \u001b[38;5;66;03m# Process the audio file\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_values\u001b[38;5;241m.\u001b[39msqueeze(), torch\u001b[38;5;241m.\u001b[39mtensor(label)\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:297\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    289\u001b[0m     uri: InputType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torchaudio/_backend/ffmpeg.py:88\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(src, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvorbis\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mogg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s\u001b[38;5;241m.\u001b[39mget_src_stream_info(s\u001b[38;5;241m.\u001b[39mdefault_audio_stream)\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n",
      "File \u001b[0;32m/apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/io/_streaming_media_decoder.py:526\u001b[0m, in \u001b[0;36mStreamingMediaDecoder.__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m ffmpeg_ext\u001b[38;5;241m.\u001b[39mStreamingMediaDecoderFileObj(src, \u001b[38;5;28mformat\u001b[39m, option, buffer_size)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamingMediaDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be\u001b[38;5;241m.\u001b[39mfind_best_audio_stream()\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_audio_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m i\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"a\" (No such file or directory).\nException raised from get_input_format_context at /__w/_temp/conda_environment_7505053768/conda-bld/torchaudio_1705078604992/work/src/libtorio/ffmpeg/stream_reader/stream_reader.cpp:42 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x151bd33a9d87 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x151bd335a75f in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torch/lib/libc10.so)\nframe #2: <unknown function> + 0x42904 (0x151b6facb904 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #3: torio::io::StreamingMediaDecoder::StreamingMediaDecoder(std::string const&, std::optional<std::string> const&, std::optional<std::map<std::string, std::string, std::less<std::string>, std::allocator<std::pair<std::string const, std::string> > > > const&) + 0x14 (0x151b6face304 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/libtorio_ffmpeg6.so)\nframe #4: <unknown function> + 0x3a58e (0x151b65c9f58e in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/_torio_ffmpeg6.so)\nframe #5: <unknown function> + 0x32147 (0x151b65c97147 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torio/lib/_torio_ffmpeg6.so)\n<omitting python frames>\nframe #11: <unknown function> + 0xf244 (0x151b748d8244 in /apps/pytorch/2.2.0/lib/python3.10/site-packages/torchaudio/lib/_torchaudio.so)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the AudioCNN class\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (input_size // 8), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to load data and extract features using Wav2Vec2\n",
    "def load_data(dataset_path, data_type, sampling_rate=16000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    model_wav2vec2 = Wav2Vec2Model.from_pretrained(\"fine_tuned_wav2vec2\")\n",
    "\n",
    "    for file_number in range(1, 51):\n",
    "        filename = f\"data_{file_number}_{data_type}.pkl\"\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                file_data = pickle.load(file)\n",
    "                for waveform, label in file_data:\n",
    "                    input_values = processor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=sampling_rate).input_values\n",
    "                    with torch.no_grad():\n",
    "                        features = model_wav2vec2(input_values).last_hidden_state.mean(dim=1).squeeze()\n",
    "                    data.append(features)\n",
    "                    labels.append(label)\n",
    "\n",
    "    labels = [x[0] for x in labels]\n",
    "    return torch.stack(data), torch.tensor(labels)\n",
    "\n",
    "# Fine-tuning Wav2Vec2 - Placeholder function\n",
    "# Implement this based on your dataset specifics\n",
    "def fine_tune_wav2vec2(train_loader, learning_rate=1e-4, num_epochs=3):\n",
    "    model_wav2vec2 = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to('cuda')\n",
    "    model_wav2vec2.train()\n",
    "    optimizer = AdamW(model_wav2vec2.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (input_values, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input_values = input_values.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "\n",
    "            outputs = model_wav2vec2(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Step {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    model_wav2vec2.save_pretrained(\"fine_tuned_wav2vec2\")\n",
    "\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, audio_paths, labels, processor):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        waveform, sr = torchaudio.load(audio_path)  # Load the audio file\n",
    "        input_values = self.processor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=sr).input_values  # Process the audio file\n",
    "\n",
    "        return input_values.squeeze(), torch.tensor(label)\n",
    "\n",
    "# Assuming you have `audio_paths` and `labels` lists prepared\n",
    "labels = [0,1,2,3,4,5,6,7,8,9]\n",
    "audio_paths = 'emi_dataset/'\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "audio_dataset = AudioDataset(audio_paths, labels, processor)\n",
    "wav2vec2_train_loader = DataLoader(audio_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Now you can fine-tune the Wav2Vec2 model\n",
    "fine_tune_wav2vec2(wav2vec2_train_loader)\n",
    "\n",
    "\n",
    "# Load and prepare data for AudioCNN training\n",
    "dataset_path = 'emi_dataset/'\n",
    "train_data, train_labels = load_data(dataset_path, \"train\")\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=32, shuffle=True)\n",
    "\n",
    "validate_data, validate_labels = load_data(dataset_path, \"valid\")\n",
    "validate_loader = DataLoader(TensorDataset(validate_data, validate_labels), batch_size=32)\n",
    "\n",
    "test_data, test_labels = load_data(dataset_path, \"test\")\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=32)\n",
    "\n",
    "# Initialize the AudioCNN model\n",
    "num_classes = 10  # Update based on your dataset\n",
    "input_size = 1024  # Update if necessary\n",
    "audio_cnn = AudioCNN(num_classes=num_classes, input_size=input_size)\n",
    "\n",
    "# AudioCNN training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(audio_cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    audio_cnn.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = audio_cnn(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    audio_cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in validate_loader:\n",
    "        outputs = audio_cnn(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(validate_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(100 * correct / total):.2f}%')\n",
    "\n",
    "# Add a similar test loop to evaluate the model on the test set\n",
    "\n",
    "\n",
    "# Test loop for AudioCNN\n",
    "audio_cnn.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for features, labels in test_loader:\n",
    "        outputs = audio_cnn(features)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest output as the prediction\n",
    "        total += labels.size(0)  # Total number of labels\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "test_loss /= len(test_loader)  # Average loss\n",
    "test_accuracy = 100 * correct / total  # Calculate accuracy\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
