{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600617c7-6a63-422a-b636-d2b53a4f56fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step 1/59, Loss: 2.2995\n",
      "Epoch 1/30, Step 11/59, Loss: 2.2884\n",
      "Epoch 1/30, Step 21/59, Loss: 2.2930\n",
      "Epoch 1/30, Step 31/59, Loss: 2.2871\n",
      "Epoch 1/30, Step 41/59, Loss: 2.3117\n",
      "Epoch 1/30, Step 51/59, Loss: 2.2874\n",
      "Epoch 1, Validation Loss: 2.2515, Validation Accuracy: 15.83%\n",
      "Epoch 2/30, Step 1/59, Loss: 2.2108\n",
      "Epoch 2/30, Step 11/59, Loss: 2.2359\n",
      "Epoch 2/30, Step 21/59, Loss: 2.2879\n",
      "Epoch 2/30, Step 31/59, Loss: 2.1529\n",
      "Epoch 2/30, Step 41/59, Loss: 2.1713\n",
      "Epoch 2/30, Step 51/59, Loss: 2.2046\n",
      "Epoch 2, Validation Loss: 2.0726, Validation Accuracy: 27.67%\n",
      "Epoch 3/30, Step 1/59, Loss: 1.9454\n",
      "Epoch 3/30, Step 11/59, Loss: 1.9668\n",
      "Epoch 3/30, Step 21/59, Loss: 1.9172\n",
      "Epoch 3/30, Step 31/59, Loss: 2.1058\n",
      "Epoch 3/30, Step 41/59, Loss: 1.8933\n",
      "Epoch 3/30, Step 51/59, Loss: 2.0971\n",
      "Epoch 3, Validation Loss: 1.8483, Validation Accuracy: 36.50%\n",
      "Epoch 4/30, Step 1/59, Loss: 1.5241\n",
      "Epoch 4/30, Step 11/59, Loss: 1.7030\n",
      "Epoch 4/30, Step 21/59, Loss: 1.7838\n",
      "Epoch 4/30, Step 31/59, Loss: 1.9866\n",
      "Epoch 4/30, Step 41/59, Loss: 1.7544\n",
      "Epoch 4/30, Step 51/59, Loss: 1.7303\n",
      "Epoch 4, Validation Loss: 1.7212, Validation Accuracy: 42.50%\n",
      "Epoch 5/30, Step 1/59, Loss: 1.6620\n",
      "Epoch 5/30, Step 11/59, Loss: 1.7534\n",
      "Epoch 5/30, Step 21/59, Loss: 1.7422\n",
      "Epoch 5/30, Step 31/59, Loss: 1.7811\n",
      "Epoch 5/30, Step 41/59, Loss: 1.8022\n",
      "Epoch 5/30, Step 51/59, Loss: 1.3247\n",
      "Epoch 5, Validation Loss: 1.6443, Validation Accuracy: 46.00%\n",
      "Epoch 6/30, Step 1/59, Loss: 1.5020\n",
      "Epoch 6/30, Step 11/59, Loss: 1.5974\n",
      "Epoch 6/30, Step 21/59, Loss: 1.3842\n",
      "Epoch 6/30, Step 31/59, Loss: 1.7035\n",
      "Epoch 6/30, Step 41/59, Loss: 1.2069\n",
      "Epoch 6/30, Step 51/59, Loss: 1.5078\n",
      "Epoch 6, Validation Loss: 1.5777, Validation Accuracy: 49.00%\n",
      "Epoch 7/30, Step 1/59, Loss: 1.2849\n",
      "Epoch 7/30, Step 11/59, Loss: 1.5034\n",
      "Epoch 7/30, Step 21/59, Loss: 1.2833\n",
      "Epoch 7/30, Step 31/59, Loss: 1.2116\n",
      "Epoch 7/30, Step 41/59, Loss: 1.3732\n",
      "Epoch 7/30, Step 51/59, Loss: 1.3055\n",
      "Epoch 7, Validation Loss: 1.6209, Validation Accuracy: 47.50%\n",
      "Epoch 8/30, Step 1/59, Loss: 1.4068\n",
      "Epoch 8/30, Step 11/59, Loss: 1.2399\n",
      "Epoch 8/30, Step 21/59, Loss: 1.3578\n",
      "Epoch 8/30, Step 31/59, Loss: 0.8752\n",
      "Epoch 8/30, Step 41/59, Loss: 1.2940\n",
      "Epoch 8/30, Step 51/59, Loss: 1.0892\n",
      "Epoch 8, Validation Loss: 1.5172, Validation Accuracy: 50.17%\n",
      "Epoch 9/30, Step 1/59, Loss: 1.1252\n",
      "Epoch 9/30, Step 11/59, Loss: 1.0194\n",
      "Epoch 9/30, Step 21/59, Loss: 0.9610\n",
      "Epoch 9/30, Step 31/59, Loss: 0.8298\n",
      "Epoch 9/30, Step 41/59, Loss: 1.2422\n",
      "Epoch 9/30, Step 51/59, Loss: 1.3075\n",
      "Epoch 9, Validation Loss: 1.5419, Validation Accuracy: 50.33%\n",
      "Epoch 10/30, Step 1/59, Loss: 1.1747\n",
      "Epoch 10/30, Step 11/59, Loss: 1.5062\n",
      "Epoch 10/30, Step 21/59, Loss: 1.0897\n",
      "Epoch 10/30, Step 31/59, Loss: 1.2507\n",
      "Epoch 10/30, Step 41/59, Loss: 1.0894\n",
      "Epoch 10/30, Step 51/59, Loss: 1.4141\n",
      "Epoch 10, Validation Loss: 1.4897, Validation Accuracy: 51.50%\n",
      "Epoch 11/30, Step 1/59, Loss: 1.0835\n",
      "Epoch 11/30, Step 11/59, Loss: 1.2099\n",
      "Epoch 11/30, Step 21/59, Loss: 1.1603\n",
      "Epoch 11/30, Step 31/59, Loss: 0.9913\n",
      "Epoch 11/30, Step 41/59, Loss: 1.2437\n",
      "Epoch 11/30, Step 51/59, Loss: 1.1424\n",
      "Epoch 11, Validation Loss: 1.5124, Validation Accuracy: 50.00%\n",
      "Epoch 12/30, Step 1/59, Loss: 1.0416\n",
      "Epoch 12/30, Step 11/59, Loss: 0.8212\n",
      "Epoch 12/30, Step 21/59, Loss: 1.0541\n",
      "Epoch 12/30, Step 31/59, Loss: 1.4559\n",
      "Epoch 12/30, Step 41/59, Loss: 1.2698\n",
      "Epoch 12/30, Step 51/59, Loss: 0.9644\n",
      "Epoch 12, Validation Loss: 1.4608, Validation Accuracy: 52.50%\n",
      "Epoch 13/30, Step 1/59, Loss: 1.2511\n",
      "Epoch 13/30, Step 11/59, Loss: 1.0816\n",
      "Epoch 13/30, Step 21/59, Loss: 1.1350\n",
      "Epoch 13/30, Step 31/59, Loss: 0.8349\n",
      "Epoch 13/30, Step 41/59, Loss: 1.2052\n",
      "Epoch 13/30, Step 51/59, Loss: 0.9630\n",
      "Epoch 13, Validation Loss: 1.5447, Validation Accuracy: 52.67%\n",
      "Epoch 14/30, Step 1/59, Loss: 0.8298\n",
      "Epoch 14/30, Step 11/59, Loss: 0.6020\n",
      "Epoch 14/30, Step 21/59, Loss: 0.6557\n",
      "Epoch 14/30, Step 31/59, Loss: 0.7873\n",
      "Epoch 14/30, Step 41/59, Loss: 0.9185\n",
      "Epoch 14/30, Step 51/59, Loss: 1.0595\n",
      "Epoch 14, Validation Loss: 1.4511, Validation Accuracy: 52.50%\n",
      "Epoch 15/30, Step 1/59, Loss: 0.8597\n",
      "Epoch 15/30, Step 11/59, Loss: 0.8228\n",
      "Epoch 15/30, Step 21/59, Loss: 0.6294\n",
      "Epoch 15/30, Step 31/59, Loss: 0.8136\n",
      "Epoch 15/30, Step 41/59, Loss: 0.8878\n",
      "Epoch 15/30, Step 51/59, Loss: 0.7058\n",
      "Epoch 15, Validation Loss: 1.5338, Validation Accuracy: 52.83%\n",
      "Epoch 16/30, Step 1/59, Loss: 0.7034\n",
      "Epoch 16/30, Step 11/59, Loss: 0.5574\n",
      "Epoch 16/30, Step 21/59, Loss: 0.5466\n",
      "Epoch 16/30, Step 31/59, Loss: 1.1300\n",
      "Epoch 16/30, Step 41/59, Loss: 0.8062\n",
      "Epoch 16/30, Step 51/59, Loss: 0.4853\n",
      "Epoch 16, Validation Loss: 1.6325, Validation Accuracy: 51.00%\n",
      "Epoch 17/30, Step 1/59, Loss: 0.7061\n",
      "Epoch 17/30, Step 11/59, Loss: 0.9387\n",
      "Epoch 17/30, Step 21/59, Loss: 0.5557\n",
      "Epoch 17/30, Step 31/59, Loss: 0.4800\n",
      "Epoch 17/30, Step 41/59, Loss: 0.8290\n",
      "Epoch 17/30, Step 51/59, Loss: 0.4547\n",
      "Epoch 17, Validation Loss: 1.6411, Validation Accuracy: 51.17%\n",
      "Epoch 18/30, Step 1/59, Loss: 0.7129\n",
      "Epoch 18/30, Step 11/59, Loss: 0.7580\n",
      "Epoch 18/30, Step 21/59, Loss: 0.4317\n",
      "Epoch 18/30, Step 31/59, Loss: 0.4584\n",
      "Epoch 18/30, Step 41/59, Loss: 0.6831\n",
      "Epoch 18/30, Step 51/59, Loss: 0.5100\n",
      "Epoch 18, Validation Loss: 1.6356, Validation Accuracy: 53.83%\n",
      "Epoch 19/30, Step 1/59, Loss: 0.4375\n",
      "Epoch 19/30, Step 11/59, Loss: 0.5345\n",
      "Epoch 19/30, Step 21/59, Loss: 0.4541\n",
      "Epoch 19/30, Step 31/59, Loss: 0.4329\n",
      "Epoch 19/30, Step 41/59, Loss: 0.3210\n",
      "Epoch 19/30, Step 51/59, Loss: 0.2947\n",
      "Epoch 19, Validation Loss: 1.7017, Validation Accuracy: 53.00%\n",
      "Epoch 20/30, Step 1/59, Loss: 0.6811\n",
      "Epoch 20/30, Step 11/59, Loss: 0.6269\n",
      "Epoch 20/30, Step 21/59, Loss: 0.4833\n",
      "Epoch 20/30, Step 31/59, Loss: 0.6270\n",
      "Epoch 20/30, Step 41/59, Loss: 0.5297\n",
      "Epoch 20/30, Step 51/59, Loss: 0.3985\n",
      "Epoch 20, Validation Loss: 1.8987, Validation Accuracy: 50.00%\n",
      "Epoch 21/30, Step 1/59, Loss: 0.8189\n",
      "Epoch 21/30, Step 11/59, Loss: 0.2915\n",
      "Epoch 21/30, Step 21/59, Loss: 0.7035\n",
      "Epoch 21/30, Step 31/59, Loss: 0.6886\n",
      "Epoch 21/30, Step 41/59, Loss: 0.6363\n",
      "Epoch 21/30, Step 51/59, Loss: 0.3017\n",
      "Epoch 21, Validation Loss: 1.7455, Validation Accuracy: 53.83%\n",
      "Epoch 22/30, Step 1/59, Loss: 0.5147\n",
      "Epoch 22/30, Step 11/59, Loss: 0.3064\n",
      "Epoch 22/30, Step 21/59, Loss: 0.4456\n",
      "Epoch 22/30, Step 31/59, Loss: 0.5683\n",
      "Epoch 22/30, Step 41/59, Loss: 0.3772\n",
      "Epoch 22/30, Step 51/59, Loss: 0.2662\n",
      "Epoch 22, Validation Loss: 1.9380, Validation Accuracy: 52.33%\n",
      "Epoch 23/30, Step 1/59, Loss: 0.3683\n",
      "Epoch 23/30, Step 11/59, Loss: 0.6798\n",
      "Epoch 23/30, Step 21/59, Loss: 0.3068\n",
      "Epoch 23/30, Step 31/59, Loss: 0.3044\n",
      "Epoch 23/30, Step 41/59, Loss: 0.1878\n",
      "Epoch 23/30, Step 51/59, Loss: 0.2713\n",
      "Epoch 23, Validation Loss: 1.9617, Validation Accuracy: 55.00%\n",
      "Epoch 24/30, Step 1/59, Loss: 0.1770\n",
      "Epoch 24/30, Step 11/59, Loss: 0.2502\n",
      "Epoch 24/30, Step 21/59, Loss: 0.4549\n",
      "Epoch 24/30, Step 31/59, Loss: 0.4290\n",
      "Epoch 24/30, Step 41/59, Loss: 0.5723\n",
      "Epoch 24/30, Step 51/59, Loss: 0.2568\n",
      "Epoch 24, Validation Loss: 2.1902, Validation Accuracy: 50.00%\n",
      "Epoch 25/30, Step 1/59, Loss: 0.1199\n",
      "Epoch 25/30, Step 11/59, Loss: 0.3327\n",
      "Epoch 25/30, Step 21/59, Loss: 0.2243\n",
      "Epoch 25/30, Step 31/59, Loss: 0.4055\n",
      "Epoch 25/30, Step 41/59, Loss: 0.3560\n",
      "Epoch 25/30, Step 51/59, Loss: 0.1603\n",
      "Epoch 25, Validation Loss: 2.0960, Validation Accuracy: 53.50%\n",
      "Epoch 26/30, Step 1/59, Loss: 0.3489\n",
      "Epoch 26/30, Step 11/59, Loss: 0.6229\n",
      "Epoch 26/30, Step 21/59, Loss: 0.4518\n",
      "Epoch 26/30, Step 31/59, Loss: 0.3608\n",
      "Epoch 26/30, Step 41/59, Loss: 0.6509\n",
      "Epoch 26/30, Step 51/59, Loss: 0.4107\n",
      "Epoch 26, Validation Loss: 1.9546, Validation Accuracy: 54.33%\n",
      "Epoch 27/30, Step 1/59, Loss: 0.1091\n",
      "Epoch 27/30, Step 11/59, Loss: 0.3278\n",
      "Epoch 27/30, Step 21/59, Loss: 0.1876\n",
      "Epoch 27/30, Step 31/59, Loss: 0.0679\n",
      "Epoch 27/30, Step 41/59, Loss: 0.1682\n",
      "Epoch 27/30, Step 51/59, Loss: 0.2038\n",
      "Epoch 27, Validation Loss: 2.1444, Validation Accuracy: 52.83%\n",
      "Epoch 28/30, Step 1/59, Loss: 0.1463\n",
      "Epoch 28/30, Step 11/59, Loss: 0.1697\n",
      "Epoch 28/30, Step 21/59, Loss: 0.1408\n",
      "Epoch 28/30, Step 31/59, Loss: 0.0605\n",
      "Epoch 28/30, Step 41/59, Loss: 0.1901\n",
      "Epoch 28/30, Step 51/59, Loss: 0.1052\n",
      "Epoch 28, Validation Loss: 2.3476, Validation Accuracy: 52.67%\n",
      "Epoch 29/30, Step 1/59, Loss: 0.2596\n",
      "Epoch 29/30, Step 11/59, Loss: 0.0690\n",
      "Epoch 29/30, Step 21/59, Loss: 0.2593\n",
      "Epoch 29/30, Step 31/59, Loss: 0.0627\n",
      "Epoch 29/30, Step 41/59, Loss: 0.1525\n",
      "Epoch 29/30, Step 51/59, Loss: 0.0587\n",
      "Epoch 29, Validation Loss: 2.2893, Validation Accuracy: 55.17%\n",
      "Epoch 30/30, Step 1/59, Loss: 0.0431\n",
      "Epoch 30/30, Step 11/59, Loss: 0.0694\n",
      "Epoch 30/30, Step 21/59, Loss: 0.0552\n",
      "Epoch 30/30, Step 31/59, Loss: 0.1783\n",
      "Epoch 30/30, Step 41/59, Loss: 0.0381\n",
      "Epoch 30/30, Step 51/59, Loss: 0.1651\n",
      "Epoch 30, Validation Loss: 2.2948, Validation Accuracy: 54.33%\n",
      "Test Loss: 2.1981, Test Accuracy: 53.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define the AudioCNN class with adjustments for input size\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (input_size // 8), 128)  # Adjusted for input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension if it's not there\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to load data and extract features using Wav2Vec2\n",
    "def load_data(dataset_path, data_type, sampling_rate=16000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    model_wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "    for file_number in range(1, 51):\n",
    "        filename = f\"data_{file_number}_{data_type}.pkl\"\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as file:\n",
    "                file_data = pickle.load(file)\n",
    "                for waveform, label in file_data:\n",
    "                    input_values = processor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=sampling_rate).input_values\n",
    "                    with torch.no_grad():\n",
    "                        features = model_wav2vec2(input_values).last_hidden_state.mean(dim=1).squeeze()\n",
    "                    data.append(features)\n",
    "                    labels.append(label)\n",
    "\n",
    "    labels = [x[0] for x in labels]\n",
    "    return torch.stack(data), torch.tensor(labels)\n",
    "\n",
    "# Load and prepare data\n",
    "dataset_path = 'emi_dataset/'\n",
    "train_data, train_labels = load_data(dataset_path, \"train\")\n",
    "train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=32, shuffle=True)\n",
    "\n",
    "validate_data, validate_labels = load_data(dataset_path, \"valid\")\n",
    "validate_loader = DataLoader(TensorDataset(validate_data, validate_labels), batch_size=32)\n",
    "\n",
    "test_data, test_labels = load_data(dataset_path, \"test\")\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 10  # Update this based on your dataset\n",
    "input_size = 1024  # Feature dimension of Wav2Vec2\n",
    "audio_cnn = AudioCNN(num_classes=num_classes, input_size=input_size)\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(audio_cnn.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30  # You can adjust the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    audio_cnn.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = audio_cnn(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    audio_cnn.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in validate_loader:\n",
    "        outputs = audio_cnn(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(validate_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(100 * correct / total):.2f}%')\n",
    "\n",
    "# Test loop\n",
    "audio_cnn.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "for features, labels in test_loader:\n",
    "    outputs = audio_cnn(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {(100 * correct / total):.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
